# ü¶úÔ∏èüîó Langchain "converse com..." Demo

![Python](https://img.shields.io/badge/python-3.10-blue)

- [ü¶úÔ∏èüîó Langchain "converse com..." Demo](#Ô∏è-langchain-converse-com-demo)
  - [Introdu√ß√£o](#introdu√ß√£o)
  - [Instala√ß√£o](#instala√ß√£o)
  - [Main](#main)
  - [File QA / Talk With Your Documents](#file-qa--talk-with-your-documents)
  - [Website QA / Talk With Your Website](#website-qa--talk-with-your-website)
  - [Duck Duck go](#duck-duck-go)

O objetivo do projeto √© explorar algumas fun√ß√µes do langchain, comparar respostas do ChatGPT com as obtidas com modelos menores (mDeBERTa) e testar uma maneira mais barata de usar o ChatGPT para responder perguntas relacionadas a um site.

## Introdu√ß√£o

Os modelos utilizados nesse projeto foram o ChatGPT-3.5 turbo e o [mDeBERTa](https://huggingface.co/timpal0l/mdeberta-v3-base-squad2). Este √∫ltimo apenas para responder perguntas com base em documentos fornecidos.

Neste aplicativo temos uma s√©rie de modelos de QA (_question answering_), isto √©, cada p√°gina tem a fun√ß√£o de responder perguntas com base em um contexto fornecido pelo pr√≥prio usu√°rio. Estes contextos podem ser de arquivo ou sites.

Praticamente todas as funcionalidades seguem o j√° conhecido fluxo de QA : 1. Os textos s√£o quebrados em peda√ßos e transformados em embeddings. 2. Um indexador cria os √≠ndices para esses embeddings e o armazena no que chamamos de base de conhecimento (_knowledge base_).  3. Quando o usu√°rio faz uma pergunta, uma busca sem√¢ntica √© feita na nossa base de conhecimento que retorna os trechos mais prov√°veis de conter a resposta. 4. Esses trechos s√£o ent√£o enviados, junto a pergunta do usu√°rio, para modelo QA que retorna a resposta final ao usu√°rio. A imagem abaixo ilustra esse fluxo.

<div style="text-align: center;">
    <img src="assets\qa_flow.png" alt="QA flow">
</div>
<p style="text-align: center;">
    Fluxo Question Answering
</p>

Utilizei um sistema id√™ntico a esse para responder perguntas com base em um site fornecido pelo usu√°rio, nesse cen√°rio o documento que comp√µe o embedding √© o conte√∫do do link que o usu√°rio fornece. 

Com exce√ß√£o do Duck Duck Go. Neste caso √© feito uma busca na web utilizando a API do Duck Duck Go com a restri√ß√£o de que o resultado deve ter o mesmo dom√≠nio informado pelo usu√°rio. Mais detalhes sobre o sistema pode ser encontrado na se√ß√£o [Duck Duck go](#duck-duck-go).

## Instala√ß√£o

Neste projeto fizemos uso do pipenv e do docker. Se n√£o tiver o docker instalado, aqui est√£o os links para instala√ß√£o para [linux](https://docs.docker.com/desktop/install/linux-install/) e [windows](https://docs.docker.com/desktop/install/windows-install/). Se j√° tiver o docker instalado √© s√≥ rodar no terminal:

```console
docker-compose build
```

o processo demora um pouco quando executado pela primeira vez. Em seguida:

```console
docker-compose up
```

Se tudo correr bem, aparecer√° a mensagem "_You can now view your Streamlit app in your browser._". Ent√£o √© s√≥ digitar no navegador: "<http://localhost:8001/>" para acessar o aplicativo.

## Main

A p√°gina inicial (main) √© simplesmente o chatgpt-3.5 turbo. Basta inserir sua OpenAI Key para poder utilizar.

## File QA / Talk With Your Documents

Na p√°gina _File QA_ √© poss√≠vel conversar com esse LEIAME que j√° teve √≠ndexes gerados e est√° dispon√≠vel para os modelos. Voc√™ pode fazer qualquer pergunta sobre esse projeto.

Voc√™ tamb√©m pode enviar documentos de at√© 200mb no formato .txt ou .pdf. Para fazer o upload √© necess√°rio selecionar um ou mais modelo embedding. O "HuggingFace embedding" ser√° usado para gerar os embeddings que ser√£o indexados e estar√£o dispon√≠vel para o mDeBERTa. J√° o "OpenAi embedding" ficar√° dispon√≠vel para o ChatGPT.

Voc√™ tamb√©m pode fazer o upload de v√°rios documentos de uma s√≥ vez. Ap√≥s submeter corretamente os arquivos, j√° √© poss√≠vel conversar com os documentos, tanto os novos quanto os antigos para os quais voc√™ j√° gerou indexes. Ao final, a fonte no qual a resposta foi retirada √© informada.

## Website QA / Talk With Your Website

Nesta se√ß√£o, o usu√°rio deve fornecer um URL antes de fazer perguntas. Por padr√£o ser√° feito o scrape apenas da p√°gina fornecida, mas o usu√°rio tem a op√ß√£o de selecionar "_Entire Website_" (Todo o site), onde scrape de todo o site ser√° feito. Tenha em mente que esta a√ß√£o pode ter um custo elevado, visto que ser√° feito o embedding de todo o site utilizando o modelo "text-embedding-ada-002" da OpenAI.

Ap√≥s inserir um URL v√°lido e decidir se quer ou n√£o utilizar todo o site, voc√™ pode fazer perguntas para o website fornecido.

## Duck Duck go

O alto custo de processar um site inteiro, como mencionado na se√ß√£o anterior, pode ser contornado utilizando um modelo embedding gratuito, a priori. Por exemplo, poder√≠amos utilizar o modelo que gera embeddings para o mDeBERTa: ["intfloat/multilingual-e5-base"](https://huggingface.co/intfloat/multilingual-e5-base).

Por√©m eu decidi fazer este teste utilizando o Duck-Duck-Go Search. Como n√£o temos um modelo inteligente como o ChatGPT para traduzir as informa√ß√µes automaticamente, temos que verificar antes se o idioma do site informado e o idioma que o usu√°rio escreveu a pergunta s√£o o mesmo e, caso necess√°rio, traduzir a pergunta do usu√°rio para o mesmo idioma do site.

O fluxo ent√£o pode ser descrito nos seguintes passos. 1. traduz a query do usu√°rio, caso necess√°rio. 2. utiliza a API do Duck Duck Go para pesquisar a pergunta do usu√°rio na web com a seguinte restri√ß√£o: a resposta deve estar no mesmo dom√≠nio informado pelo usu√°rio. 3. Passamos os trechos com as respostas encontradas pelo Duck Duck Go e a pergunta do usu√°rio para o ChatGPT. 4. O modelo se encarrega de encontrar a resposta e traduzir de volta, se necess√°rio, para o idioma do usu√°rio.

<div style="text-align: center;">
    <img src="assets\duck_go_flow.png" alt="Duck Go flow">
</div>
<p style="text-align: center;">
    Fluxo do Duck Duck Go Search
</p>

Note que dessa forma, n√£o geramos os √≠ndices sem√¢nticos. O pr√≥prio Duck Duck Go se encarrega de fazer a busca pela informa√ß√£o que desejamos. Embora o modelo pare√ßa funcionar bem, ele √© mais lento que o fluxo QA. Isso acontece porque n√£o √© criada uma base de conhecimento com √≠ndices para fazermos a busca sem√¢ntica. Ou seja, a cada pergunta o fluxo inteiro √© refeito.

<style>
    body {
        text-align: justify;
    }
</style>
